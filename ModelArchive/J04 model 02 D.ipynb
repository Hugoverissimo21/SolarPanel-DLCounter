{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import Counter\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "#cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train = pd.read_pickle(r\"C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\Model_Train.pkl\")\n",
    "df_val = pd.read_pickle(r\"C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\Model_Val.pkl\")\n",
    "df_train = df_train[df_train[\"img_origin\"] == \"D\"].reset_index(drop=True)\n",
    "df_val = df_val[df_val[\"img_origin\"] == \"D\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is using a different strategy:\n",
    "- The metadata was encoded onto the images via one hot encoding \n",
    "- Based on the 2 classes and 2 origins, the class balancing was attempted for the 4 classes during the albumentations step (although officially there are only 2 classes still, solar and boiler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create multi-class mask\n",
    "def create_multi_class_mask(image_size, polygons_boil, polygons_pan):\n",
    "    mask = np.full(image_size, 1, dtype=np.uint8)  # Default background is Photovoltaic (1)\n",
    "    \n",
    "    # Draw boiler panels (0)\n",
    "    for polygon in polygons_boil:\n",
    "        cv2.fillPoly(mask, np.array([polygon], dtype=np.int32), 0)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "# One-hot encode metadata\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "metadata_encoded = encoder.fit_transform(df_train[['img_placement', 'img_origin']])\n",
    "\n",
    "# Define transformation pipelines\n",
    "albumentations_transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.GaussianBlur(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Dataset class\n",
    "class SolarPanelDataset(Dataset):\n",
    "    def __init__(self, metadata_df, image_dir, transform=None, mask_size=(512, 512), balance=False):\n",
    "        self.metadata = metadata_df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.mask_size = mask_size\n",
    "        self.balance = balance\n",
    "        \n",
    "        # One-hot encode metadata\n",
    "        self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self.encoded_metadata = self.encoder.fit_transform(self.metadata[['img_placement', 'img_origin']])\n",
    "        \n",
    "        # Create class labels for balancing\n",
    "        self.class_labels = self.metadata.apply(lambda row: f\"{row['img_origin']}_{'solar' if row['polygons_pan'] else 'boiler'}\", axis=1)\n",
    "        \n",
    "        # Compute class weights for balancing\n",
    "        if balance:\n",
    "            class_counts = Counter(self.class_labels)\n",
    "            self.weights = [1.0 / class_counts[label] for label in self.class_labels]\n",
    "        else:\n",
    "            self.weights = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        img_path = f\"{self.image_dir}/{row['img_id']}.jpg\"\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        # Create the mask\n",
    "        mask = create_multi_class_mask(image.shape[:2], row['polygons_boil'], row['polygons_pan'])\n",
    "        mask = np.array(mask, dtype=np.uint8)\n",
    "\n",
    "        # Apply transformations\n",
    "        augmented = self.transform(image=image, mask=mask)\n",
    "        image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "        # Convert mask to long tensor\n",
    "        if isinstance(mask, np.ndarray):  # Convert only if it's still a NumPy array\n",
    "            mask = torch.from_numpy(mask).long()\n",
    "        else:\n",
    "            mask = mask.long()  # If it's already a tensor, just ensure dtype\n",
    "\n",
    "        # Get one-hot encoded metadata\n",
    "        metadata_vector = torch.tensor(self.encoded_metadata[idx], dtype=torch.float32)\n",
    "\n",
    "        return image, mask, metadata_vector  # Return metadata as additional input\n",
    "\n",
    "# Define image directory\n",
    "image_dir = r\"C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\images\"\n",
    "\n",
    "# Create train dataset with class balancing\n",
    "train_dataset = SolarPanelDataset(df_train, image_dir, transform=albumentations_transform, balance=True)\n",
    "val_dataset = SolarPanelDataset(df_val, image_dir, transform=A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "]))\n",
    "\n",
    "# Create Weighted Sampler for class balancing\n",
    "if train_dataset.weights:\n",
    "    sampler = WeightedRandomSampler(weights=train_dataset.weights, num_samples=len(train_dataset), replacement=True)\n",
    "else:\n",
    "    sampler = None\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler if sampler else None, shuffle=sampler is None, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gnvca\\anaconda3\\envs\\caa01\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load DeepLabV3+ with EfficientNet-B4 backbone\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"efficientnet-b4\",  # EfficientNet-B4 as the encoder\n",
    "    encoder_weights=\"imagenet\",  # Pretrained weights\n",
    "    in_channels=3,  # RGB images\n",
    "    classes=2  # Boiler (0), Photovoltaic (1)\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Add dropout before the classifier correctly\n",
    "model.segmentation_head = nn.Sequential(\n",
    "    nn.Dropout(0.3),  # 30% dropout\n",
    "    model.segmentation_head\n",
    ")\n",
    "\n",
    "# Define loss function (CrossEntropy + Dice Loss for better performance)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dice_loss = smp.losses.DiceLoss(mode='multiclass')\n",
    "\n",
    "# Adam optimizer with weight decay\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Mixed precision scaler for faster GPU training\n",
    "scaler = torch.amp.GradScaler(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Training: 100%|██████████| 500/500 [06:30<00:00,  1.28it/s]\n",
      "Validation: 100%|██████████| 123/123 [01:05<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 1/50\n",
      "   📉 Train Loss: 0.6865 | 🏆 Train IoU: 0.4950\n",
      "   📉 Val Loss: 0.1182 | 🏆 Val IoU: 0.9593\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 Training: 100%|██████████| 500/500 [06:33<00:00,  1.27it/s]\n",
      "Validation: 100%|██████████| 123/123 [01:07<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 2/50\n",
      "   📉 Train Loss: 0.5142 | 🏆 Train IoU: 0.4999\n",
      "   📉 Val Loss: 0.0644 | 🏆 Val IoU: 0.9593\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 Training: 100%|██████████| 500/500 [06:29<00:00,  1.28it/s]\n",
      "Validation: 100%|██████████| 123/123 [01:08<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 3/50\n",
      "   📉 Train Loss: 0.4799 | 🏆 Train IoU: 0.5175\n",
      "   📉 Val Loss: 0.0526 | 🏆 Val IoU: 0.8700\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 Training: 100%|██████████| 500/500 [06:29<00:00,  1.28it/s]\n",
      "Validation: 100%|██████████| 123/123 [01:08<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 4/50\n",
      "   📉 Train Loss: 0.4633 | 🏆 Train IoU: 0.5461\n",
      "   📉 Val Loss: 0.0466 | 🏆 Val IoU: 0.8907\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 Training: 100%|██████████| 500/500 [06:31<00:00,  1.28it/s]\n",
      "Validation: 100%|██████████| 123/123 [01:12<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 5/50\n",
      "   📉 Train Loss: 0.4365 | 🏆 Train IoU: 0.5707\n",
      "   📉 Val Loss: 0.0429 | 🏆 Val IoU: 0.9431\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 Training: 100%|██████████| 500/500 [06:31<00:00,  1.28it/s]\n",
      "Validation: 100%|██████████| 123/123 [01:09<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 6/50\n",
      "   📉 Train Loss: 0.3897 | 🏆 Train IoU: 0.6060\n",
      "   📉 Val Loss: 0.0413 | 🏆 Val IoU: 0.9268\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 Training: 100%|██████████| 500/500 [06:39<00:00,  1.25it/s]\n",
      "Validation: 100%|██████████| 123/123 [01:07<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 7/50\n",
      "   📉 Train Loss: 0.3300 | 🏆 Train IoU: 0.6435\n",
      "   📉 Val Loss: 0.0407 | 🏆 Val IoU: 0.9471\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 Training: 100%|██████████| 500/500 [05:39<00:00,  1.47it/s]\n",
      "Validation: 100%|██████████| 123/123 [00:57<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 8/50\n",
      "   📉 Train Loss: 0.3003 | 🏆 Train IoU: 0.6546\n",
      "   📉 Val Loss: 0.0408 | 🏆 Val IoU: 0.9472\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 Training: 100%|██████████| 500/500 [05:03<00:00,  1.65it/s]\n",
      "Validation: 100%|██████████| 123/123 [00:55<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 9/50\n",
      "   📉 Train Loss: 0.2571 | 🏆 Train IoU: 0.6840\n",
      "   📉 Val Loss: 0.0407 | 🏆 Val IoU: 0.9553\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 Training: 100%|██████████| 500/500 [04:57<00:00,  1.68it/s]\n",
      "Validation: 100%|██████████| 123/123 [00:55<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 10/50\n",
      "   📉 Train Loss: 0.2500 | 🏆 Train IoU: 0.6869\n",
      "   📉 Val Loss: 0.0407 | 🏆 Val IoU: 0.9593\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 Training: 100%|██████████| 500/500 [05:12<00:00,  1.60it/s]\n",
      "Validation: 100%|██████████| 123/123 [00:56<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 11/50\n",
      "   📉 Train Loss: 0.2217 | 🏆 Train IoU: 0.7200\n",
      "   📉 Val Loss: 0.0403 | 🏆 Val IoU: 0.9514\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 Training: 100%|██████████| 500/500 [05:09<00:00,  1.61it/s]\n",
      "Validation: 100%|██████████| 123/123 [00:56<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 12/50\n",
      "   📉 Train Loss: 0.2047 | 🏆 Train IoU: 0.7214\n",
      "   📉 Val Loss: 0.0407 | 🏆 Val IoU: 0.9593\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 Training: 100%|██████████| 500/500 [05:13<00:00,  1.59it/s]\n",
      "Validation: 100%|██████████| 123/123 [00:58<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 13/50\n",
      "   📉 Train Loss: 0.2001 | 🏆 Train IoU: 0.7362\n",
      "   📉 Val Loss: 0.0407 | 🏆 Val IoU: 0.9593\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 Training: 100%|██████████| 500/500 [05:32<00:00,  1.50it/s]\n",
      "Validation: 100%|██████████| 123/123 [00:57<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 14/50\n",
      "   📉 Train Loss: 0.1995 | 🏆 Train IoU: 0.7389\n",
      "   📉 Val Loss: 0.0407 | 🏆 Val IoU: 0.9553\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 Training: 100%|██████████| 500/500 [06:14<00:00,  1.34it/s]\n",
      "Validation: 100%|██████████| 123/123 [00:56<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 15/50\n",
      "   📉 Train Loss: 0.1873 | 🏆 Train IoU: 0.7411\n",
      "   📉 Val Loss: 0.0406 | 🏆 Val IoU: 0.9593\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 Training:  16%|█▋        | 82/500 [00:49<04:17,  1.62it/s]"
     ]
    }
   ],
   "source": [
    "# Function to calculate IoU\n",
    "def iou_score(preds, labels, num_classes=2):\n",
    "    \"\"\"Compute IoU (Intersection over Union) for multi-class segmentation.\"\"\"\n",
    "    preds = torch.argmax(preds, dim=1)  # Convert logits to class predictions\n",
    "    iou = []\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        intersection = ((preds == cls) & (labels == cls)).sum().item()\n",
    "        union = ((preds == cls) | (labels == cls)).sum().item()\n",
    "        if union == 0:\n",
    "            iou.append(float('nan'))\n",
    "        else:\n",
    "            iou.append(intersection / union)\n",
    "\n",
    "    return np.nanmean(iou)  # Ignore NaNs if a class is missing in batch\n",
    "\n",
    "\n",
    "# 🔹 Model Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"efficientnet-b4\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=2\n",
    ").to(device)\n",
    "\n",
    "# 🔹 Add Dropout Correctly\n",
    "model.segmentation_head = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    model.segmentation_head\n",
    ")\n",
    "\n",
    "# 🔹 Loss Functions (CrossEntropy + Dice Loss)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dice_loss = smp.losses.DiceLoss(mode='multiclass')\n",
    "\n",
    "# 🔹 Optimizer & LR Scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# 🔹 Mixed Precision (Speeds up Training)\n",
    "scaler = torch.amp.GradScaler(device=\"cuda\")\n",
    "\n",
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_iou\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_iou\": []\n",
    "}\n",
    "\n",
    "output_dir = r\"C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\\"\n",
    "\n",
    "# Training Hyperparameters\n",
    "num_epochs = 50\n",
    "best_val_loss = float(\"inf\")\n",
    "accumulation_steps = 4  # Simulates larger batch size\n",
    "\n",
    "# 🔹 Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    optimizer.zero_grad()  # Initialize gradients before accumulation\n",
    "\n",
    "    for i, (images, masks, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\", leave=True, dynamic_ncols=True)):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):  # Enables mixed precision\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, masks) + dice_loss(outputs, masks)  # Combined loss\n",
    "        \n",
    "        scaler.scale(loss).backward()  # Accumulate gradients\n",
    "\n",
    "        # 🔹 Only update every `accumulation_steps`\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total_iou += iou_score(outputs, masks, num_classes=2)\n",
    "        num_batches += 1\n",
    "        \n",
    "        #tqdm.write(f\"Batch {i+1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = running_loss / num_batches\n",
    "    avg_train_iou = total_iou / num_batches\n",
    "\n",
    "    # 🔹 Validation Loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iou = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks, _ in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):  # Use mixed precision in inference\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks) + dice_loss(outputs, masks)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_iou += iou_score(outputs, masks, num_classes=2)\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_val_loss = val_loss / num_batches\n",
    "    avg_val_iou = val_iou / num_batches\n",
    "\n",
    "    # 🔥 Save Best Model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), f\"bm_effdeepl_D_epoch{epoch}.pth\")\n",
    "        print(\"🔥 Best Model Saved!\")\n",
    "\n",
    "    # 🔹 Logging\n",
    "    print(f\"\\n🔹 Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"   📉 Train Loss: {avg_train_loss:.4f} | 🏆 Train IoU: {avg_train_iou:.4f}\")\n",
    "    print(f\"   📉 Val Loss: {avg_val_loss:.4f} | 🏆 Val IoU: {avg_val_iou:.4f}\")\n",
    "\n",
    "    history[\"epoch\"].append(epoch + 1)\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"train_iou\"].append(avg_train_iou)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"val_iou\"].append(avg_val_iou)\n",
    "\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_path = os.path.join(output_dir, \"training_history_02_D.csv\")\n",
    "    history_df.to_csv(history_path, index=False)\n",
    "    print(f\"📊 Training history saved to: {history_path}\")\n",
    "    \n",
    "    # 🔹 Adjust LR based on Validation Loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is very promising, with a substantial IoU in comparison to the combined approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train = pd.read_pickle(r\"C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\Model_Train.pkl\")\n",
    "df_val = pd.read_pickle(r\"C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\Model_Val.pkl\")\n",
    "df_train = df_train[df_train[\"img_origin\"] == \"S\"].reset_index(drop=True)\n",
    "df_val = df_val[df_val[\"img_origin\"] == \"S\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Grouped count of images with multiple panels:\n",
      "has_multiple_panels\n",
      "False    296\n",
      "True     145\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert stringified lists to actual lists if needed\n",
    "import ast\n",
    "\n",
    "def safe_eval(val):\n",
    "    if isinstance(val, str):\n",
    "        return ast.literal_eval(val)\n",
    "    return val\n",
    "\n",
    "df_train[\"polygons_boil\"] = df_train[\"polygons_boil\"].apply(safe_eval)\n",
    "df_train[\"polygons_pan\"] = df_train[\"polygons_pan\"].apply(safe_eval)\n",
    "\n",
    "# Count the number of images with more than 1 polygon in either field\n",
    "def has_multiple_panels(polygons_boil, polygons_pan):\n",
    "    return len(polygons_boil) > 1 or len(polygons_pan) > 1\n",
    "\n",
    "df_train[\"has_multiple_panels\"] = df_train.apply(\n",
    "    lambda row: has_multiple_panels(row[\"polygons_boil\"], row[\"polygons_pan\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Group and count\n",
    "panel_counts = df_train[\"has_multiple_panels\"].value_counts()\n",
    "\n",
    "print(\"✅ Grouped count of images with multiple panels:\")\n",
    "print(panel_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model with Dropout segmentation head loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Rebuild model architecture exactly as before\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"efficientnet-b4\",\n",
    "    encoder_weights=None,  # Don't load ImageNet again\n",
    "    in_channels=3,\n",
    "    classes=2\n",
    ")\n",
    "\n",
    "# Add dropout before loading weights (must match original architecture)\n",
    "model.segmentation_head = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    model.segmentation_head\n",
    ")\n",
    "\n",
    "# Load weights (strict=True now that it matches)\n",
    "checkpoint_path = r\"C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\model02D\\bm_effdeepl_D_epoch10.pth\"\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=True)\n",
    "\n",
    "# Move to device\n",
    "model.to(device)\n",
    "\n",
    "print(\"✅ Model with Dropout segmentation head loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_batchnorm_to_groupnorm(model, num_groups=32):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.BatchNorm2d):\n",
    "            setattr(model, name, torch.nn.GroupNorm(num_groups, module.num_features))\n",
    "        else:\n",
    "            convert_batchnorm_to_groupnorm(module, num_groups)\n",
    "\n",
    "# Function to create multi-class mask\n",
    "def create_multi_class_mask(image_size, polygons_boil, polygons_pan):\n",
    "    mask = np.full(image_size, 1, dtype=np.uint8)  # Default background is Photovoltaic (1)\n",
    "    \n",
    "    # Draw boiler panels (0)\n",
    "    for polygon in polygons_boil:\n",
    "        cv2.fillPoly(mask, np.array([polygon], dtype=np.int32), 0)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "# One-hot encode metadata\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "metadata_encoded = encoder.fit_transform(df_train[['img_placement', 'img_origin']])\n",
    "\n",
    "# Define transformation pipelines\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Resize(512, 512),  # ✅ Ensure final size is large enough\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "# Define transformation for training\n",
    "albumentations_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Resize(512, 512),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Dataset class\n",
    "class SolarPanelDataset(Dataset):\n",
    "    def __init__(self, metadata_df, image_dir, transform=None, mask_size=(512, 512), balance=False):\n",
    "        self.metadata = metadata_df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.mask_size = mask_size\n",
    "        self.balance = balance\n",
    "        \n",
    "        # One-hot encode metadata\n",
    "        self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self.encoded_metadata = self.encoder.fit_transform(self.metadata[['img_placement', 'img_origin']])\n",
    "        \n",
    "        # Create class labels for balancing\n",
    "        self.class_labels = self.metadata.apply(lambda row: f\"{row['img_origin']}_{'solar' if row['polygons_pan'] else 'boiler'}\", axis=1)\n",
    "        \n",
    "        # Compute class weights for balancing\n",
    "        if balance:\n",
    "            class_counts = Counter(self.class_labels)\n",
    "            self.weights = [1.0 / class_counts[label] for label in self.class_labels]\n",
    "        else:\n",
    "            self.weights = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        img_path = f\"{self.image_dir}/{row['img_id']}.jpg\"\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        # Create the mask\n",
    "        mask = create_multi_class_mask(image.shape[:2], row['polygons_boil'], row['polygons_pan'])\n",
    "        mask = np.array(mask, dtype=np.uint8)\n",
    "\n",
    "        # Apply transformations\n",
    "        augmented = self.transform(image=image, mask=mask)\n",
    "        image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "        # Convert mask to long tensor\n",
    "        if isinstance(mask, np.ndarray):  # Convert only if it's still a NumPy array\n",
    "            mask = torch.from_numpy(mask).long()\n",
    "        else:\n",
    "            mask = mask.long()  # If it's already a tensor, just ensure dtype\n",
    "\n",
    "        # Get one-hot encoded metadata\n",
    "        metadata_vector = torch.tensor(self.encoded_metadata[idx], dtype=torch.float32)\n",
    "\n",
    "        return image, mask, metadata_vector  # Return metadata as additional input\n",
    "\n",
    "# Define image directory\n",
    "image_dir = r\"C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\images\"\n",
    "\n",
    "# Create train dataset with class balancing\n",
    "train_dataset = SolarPanelDataset(df_train, image_dir, transform=albumentations_transform, balance=True)\n",
    "val_dataset = SolarPanelDataset(df_val, image_dir, transform=A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "]))\n",
    "\n",
    "# Create Weighted Sampler for class balancing\n",
    "if train_dataset.weights:\n",
    "    sampler = WeightedRandomSampler(weights=train_dataset.weights, num_samples=len(train_dataset), replacement=True)\n",
    "else:\n",
    "    sampler = None\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler if sampler else None, shuffle=sampler is None, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gnvca\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch 1/50 Training: 100%|██████████| 28/28 [04:25<00:00,  9.48s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 1/50\n",
      "   📉 Train Loss: 0.5280 | 🏆 Train IoU: 0.5043\n",
      "   📉 Val Loss: 0.5501 | 🏆 Val IoU: 0.4972\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 Training: 100%|██████████| 28/28 [04:27<00:00,  9.56s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 2/50\n",
      "   📉 Train Loss: 0.4961 | 🏆 Train IoU: 0.5306\n",
      "   📉 Val Loss: 0.5368 | 🏆 Val IoU: 0.4980\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 Training: 100%|██████████| 28/28 [04:20<00:00,  9.31s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 3/50\n",
      "   📉 Train Loss: 0.5001 | 🏆 Train IoU: 0.5266\n",
      "   📉 Val Loss: 0.5326 | 🏆 Val IoU: 0.4990\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 Training: 100%|██████████| 28/28 [04:20<00:00,  9.31s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 4/50\n",
      "   📉 Train Loss: 0.4912 | 🏆 Train IoU: 0.5327\n",
      "   📉 Val Loss: 0.5306 | 🏆 Val IoU: 0.5030\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 Training: 100%|██████████| 28/28 [04:22<00:00,  9.37s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 5/50\n",
      "   📉 Train Loss: 0.4824 | 🏆 Train IoU: 0.5403\n",
      "   📉 Val Loss: 0.5284 | 🏆 Val IoU: 0.5062\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.35s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 6/50\n",
      "   📉 Train Loss: 0.4718 | 🏆 Train IoU: 0.5436\n",
      "   📉 Val Loss: 0.5284 | 🏆 Val IoU: 0.5115\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 Training: 100%|██████████| 28/28 [04:22<00:00,  9.37s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 7/50\n",
      "   📉 Train Loss: 0.4611 | 🏆 Train IoU: 0.5508\n",
      "   📉 Val Loss: 0.5248 | 🏆 Val IoU: 0.5069\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 Training: 100%|██████████| 28/28 [04:22<00:00,  9.38s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 8/50\n",
      "   📉 Train Loss: 0.4478 | 🏆 Train IoU: 0.5600\n",
      "   📉 Val Loss: 0.5202 | 🏆 Val IoU: 0.5155\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.33s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 9/50\n",
      "   📉 Train Loss: 0.4310 | 🏆 Train IoU: 0.5713\n",
      "   📉 Val Loss: 0.5280 | 🏆 Val IoU: 0.5063\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.34s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 10/50\n",
      "   📉 Train Loss: 0.4585 | 🏆 Train IoU: 0.5507\n",
      "   📉 Val Loss: 0.5271 | 🏆 Val IoU: 0.5053\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.33s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 11/50\n",
      "   📉 Train Loss: 0.4190 | 🏆 Train IoU: 0.5788\n",
      "   📉 Val Loss: 0.5236 | 🏆 Val IoU: 0.5094\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 Training: 100%|██████████| 28/28 [04:22<00:00,  9.39s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 12/50\n",
      "   📉 Train Loss: 0.4021 | 🏆 Train IoU: 0.5896\n",
      "   📉 Val Loss: 0.5326 | 🏆 Val IoU: 0.5040\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 Training: 100%|██████████| 28/28 [04:20<00:00,  9.29s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 13/50\n",
      "   📉 Train Loss: 0.4225 | 🏆 Train IoU: 0.5760\n",
      "   📉 Val Loss: 0.5309 | 🏆 Val IoU: 0.5118\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 Training: 100%|██████████| 28/28 [04:22<00:00,  9.38s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 14/50\n",
      "   📉 Train Loss: 0.4117 | 🏆 Train IoU: 0.5899\n",
      "   📉 Val Loss: 0.5384 | 🏆 Val IoU: 0.4989\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 Training: 100%|██████████| 28/28 [04:22<00:00,  9.36s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 15/50\n",
      "   📉 Train Loss: 0.4140 | 🏆 Train IoU: 0.5799\n",
      "   📉 Val Loss: 0.5321 | 🏆 Val IoU: 0.5034\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 Training: 100%|██████████| 28/28 [04:22<00:00,  9.38s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 16/50\n",
      "   📉 Train Loss: 0.3798 | 🏆 Train IoU: 0.6086\n",
      "   📉 Val Loss: 0.5315 | 🏆 Val IoU: 0.5042\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 Training: 100%|██████████| 28/28 [04:22<00:00,  9.38s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 17/50\n",
      "   📉 Train Loss: 0.3809 | 🏆 Train IoU: 0.6036\n",
      "   📉 Val Loss: 0.5351 | 🏆 Val IoU: 0.4997\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.33s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 18/50\n",
      "   📉 Train Loss: 0.3690 | 🏆 Train IoU: 0.6125\n",
      "   📉 Val Loss: 0.5335 | 🏆 Val IoU: 0.5012\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.35s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 19/50\n",
      "   📉 Train Loss: 0.3544 | 🏆 Train IoU: 0.6256\n",
      "   📉 Val Loss: 0.5355 | 🏆 Val IoU: 0.5010\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.34s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 20/50\n",
      "   📉 Train Loss: 0.3854 | 🏆 Train IoU: 0.6018\n",
      "   📉 Val Loss: 0.5354 | 🏆 Val IoU: 0.5004\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.35s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 21/50\n",
      "   📉 Train Loss: 0.3556 | 🏆 Train IoU: 0.6252\n",
      "   📉 Val Loss: 0.5343 | 🏆 Val IoU: 0.5004\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.33s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 22/50\n",
      "   📉 Train Loss: 0.3532 | 🏆 Train IoU: 0.6259\n",
      "   📉 Val Loss: 0.5354 | 🏆 Val IoU: 0.5002\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.33s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 23/50\n",
      "   📉 Train Loss: 0.3783 | 🏆 Train IoU: 0.6091\n",
      "   📉 Val Loss: 0.5337 | 🏆 Val IoU: 0.5007\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.36s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 24/50\n",
      "   📉 Train Loss: 0.4110 | 🏆 Train IoU: 0.5847\n",
      "   📉 Val Loss: 0.5294 | 🏆 Val IoU: 0.5056\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.33s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 25/50\n",
      "   📉 Train Loss: 0.3548 | 🏆 Train IoU: 0.6240\n",
      "   📉 Val Loss: 0.5316 | 🏆 Val IoU: 0.5030\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 Training: 100%|██████████| 28/28 [04:22<00:00,  9.37s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 26/50\n",
      "   📉 Train Loss: 0.3650 | 🏆 Train IoU: 0.6169\n",
      "   📉 Val Loss: 0.5322 | 🏆 Val IoU: 0.5023\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.34s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 27/50\n",
      "   📉 Train Loss: 0.3712 | 🏆 Train IoU: 0.6122\n",
      "   📉 Val Loss: 0.5331 | 🏆 Val IoU: 0.5023\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.33s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 28/50\n",
      "   📉 Train Loss: 0.3518 | 🏆 Train IoU: 0.6250\n",
      "   📉 Val Loss: 0.5324 | 🏆 Val IoU: 0.5029\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.32s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 29/50\n",
      "   📉 Train Loss: 0.3519 | 🏆 Train IoU: 0.6261\n",
      "   📉 Val Loss: 0.5340 | 🏆 Val IoU: 0.5016\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.34s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 30/50\n",
      "   📉 Train Loss: 0.3806 | 🏆 Train IoU: 0.6057\n",
      "   📉 Val Loss: 0.5320 | 🏆 Val IoU: 0.5030\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 Training: 100%|██████████| 28/28 [04:20<00:00,  9.30s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 31/50\n",
      "   📉 Train Loss: 0.3893 | 🏆 Train IoU: 0.6023\n",
      "   📉 Val Loss: 0.5303 | 🏆 Val IoU: 0.5046\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 Training: 100%|██████████| 28/28 [04:20<00:00,  9.31s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 32/50\n",
      "   📉 Train Loss: 0.3505 | 🏆 Train IoU: 0.6272\n",
      "   📉 Val Loss: 0.5331 | 🏆 Val IoU: 0.5019\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.32s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 33/50\n",
      "   📉 Train Loss: 0.3622 | 🏆 Train IoU: 0.6167\n",
      "   📉 Val Loss: 0.5352 | 🏆 Val IoU: 0.5004\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 Training: 100%|██████████| 28/28 [04:22<00:00,  9.38s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 34/50\n",
      "   📉 Train Loss: 0.3490 | 🏆 Train IoU: 0.6271\n",
      "   📉 Val Loss: 0.5366 | 🏆 Val IoU: 0.4988\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 Training: 100%|██████████| 28/28 [04:20<00:00,  9.32s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 35/50\n",
      "   📉 Train Loss: 0.3447 | 🏆 Train IoU: 0.6345\n",
      "   📉 Val Loss: 0.5368 | 🏆 Val IoU: 0.4988\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 Training: 100%|██████████| 28/28 [04:22<00:00,  9.36s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 36/50\n",
      "   📉 Train Loss: 0.3652 | 🏆 Train IoU: 0.6182\n",
      "   📉 Val Loss: 0.5356 | 🏆 Val IoU: 0.4997\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.35s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 37/50\n",
      "   📉 Train Loss: 0.3570 | 🏆 Train IoU: 0.6247\n",
      "   📉 Val Loss: 0.5337 | 🏆 Val IoU: 0.5007\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.33s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 38/50\n",
      "   📉 Train Loss: 0.3785 | 🏆 Train IoU: 0.6051\n",
      "   📉 Val Loss: 0.5343 | 🏆 Val IoU: 0.5015\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.32s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 39/50\n",
      "   📉 Train Loss: 0.3697 | 🏆 Train IoU: 0.6142\n",
      "   📉 Val Loss: 0.5328 | 🏆 Val IoU: 0.5027\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 Training: 100%|██████████| 28/28 [04:22<00:00,  9.38s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 40/50\n",
      "   📉 Train Loss: 0.3604 | 🏆 Train IoU: 0.6190\n",
      "   📉 Val Loss: 0.5340 | 🏆 Val IoU: 0.5020\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.33s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 41/50\n",
      "   📉 Train Loss: 0.3495 | 🏆 Train IoU: 0.6281\n",
      "   📉 Val Loss: 0.5345 | 🏆 Val IoU: 0.5010\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 Training: 100%|██████████| 28/28 [04:22<00:00,  9.36s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 42/50\n",
      "   📉 Train Loss: 0.3844 | 🏆 Train IoU: 0.6018\n",
      "   📉 Val Loss: 0.5348 | 🏆 Val IoU: 0.5001\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.34s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 43/50\n",
      "   📉 Train Loss: 0.3595 | 🏆 Train IoU: 0.6205\n",
      "   📉 Val Loss: 0.5348 | 🏆 Val IoU: 0.5000\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.33s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 44/50\n",
      "   📉 Train Loss: 0.3563 | 🏆 Train IoU: 0.6226\n",
      "   📉 Val Loss: 0.5348 | 🏆 Val IoU: 0.5002\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 Training: 100%|██████████| 28/28 [04:19<00:00,  9.28s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 45/50\n",
      "   📉 Train Loss: 0.3774 | 🏆 Train IoU: 0.6081\n",
      "   📉 Val Loss: 0.5322 | 🏆 Val IoU: 0.5020\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.35s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 46/50\n",
      "   📉 Train Loss: 0.3527 | 🏆 Train IoU: 0.6237\n",
      "   📉 Val Loss: 0.5356 | 🏆 Val IoU: 0.4999\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.34s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 47/50\n",
      "   📉 Train Loss: 0.3415 | 🏆 Train IoU: 0.6364\n",
      "   📉 Val Loss: 0.5343 | 🏆 Val IoU: 0.5004\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 Training: 100%|██████████| 28/28 [04:21<00:00,  9.33s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:18<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 48/50\n",
      "   📉 Train Loss: 0.3527 | 🏆 Train IoU: 0.6267\n",
      "   📉 Val Loss: 0.5324 | 🏆 Val IoU: 0.5025\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 Training: 100%|██████████| 28/28 [04:17<00:00,  9.21s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:16<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 49/50\n",
      "   📉 Train Loss: 0.3777 | 🏆 Train IoU: 0.6067\n",
      "   📉 Val Loss: 0.5329 | 🏆 Val IoU: 0.5023\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 Training: 100%|██████████| 28/28 [04:13<00:00,  9.07s/it]\n",
      "Validation: 100%|██████████| 8/8 [00:17<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 50/50\n",
      "   📉 Train Loss: 0.3547 | 🏆 Train IoU: 0.6218\n",
      "   📉 Val Loss: 0.5342 | 🏆 Val IoU: 0.5003\n",
      "📊 Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_D.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Function to calculate IoU\n",
    "def iou_score(preds, labels, num_classes=2):\n",
    "    \"\"\"Compute IoU (Intersection over Union) for multi-class segmentation.\"\"\"\n",
    "    preds = torch.argmax(preds, dim=1)  # Convert logits to class predictions\n",
    "    iou = []\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        intersection = ((preds == cls) & (labels == cls)).sum().item()\n",
    "        union = ((preds == cls) | (labels == cls)).sum().item()\n",
    "        if union == 0:\n",
    "            iou.append(float('nan'))\n",
    "        else:\n",
    "            iou.append(intersection / union)\n",
    "\n",
    "    return np.nanmean(iou)  # Ignore NaNs if a class is missing in batch\n",
    "\n",
    "\n",
    "# 🔹 Loss Functions (CrossEntropy + Dice Loss)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dice_loss = smp.losses.DiceLoss(mode='multiclass')\n",
    "\n",
    "# 🔹 Optimizer & LR Scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# 🔹 Mixed Precision (Speeds up Training)\n",
    "scaler = torch.amp.GradScaler(device=\"cuda\")\n",
    "\n",
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_iou\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_iou\": []\n",
    "}\n",
    "\n",
    "output_dir = r\"C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\\"\n",
    "\n",
    "# Training Hyperparameters\n",
    "num_epochs = 50\n",
    "best_val_loss = float(\"inf\")\n",
    "accumulation_steps = 4  # Simulates larger batch size\n",
    "\n",
    "# 🔹 Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    optimizer.zero_grad()  # Initialize gradients before accumulation\n",
    "\n",
    "    for i, (images, masks, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\", leave=True, dynamic_ncols=True)):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):  # Enables mixed precision\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, masks) + dice_loss(outputs, masks)  # Combined loss\n",
    "        \n",
    "        scaler.scale(loss).backward()  # Accumulate gradients\n",
    "\n",
    "        # 🔹 Only update every `accumulation_steps`\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total_iou += iou_score(outputs, masks, num_classes=2)\n",
    "        num_batches += 1\n",
    "        \n",
    "        #tqdm.write(f\"Batch {i+1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = running_loss / num_batches\n",
    "    avg_train_iou = total_iou / num_batches\n",
    "\n",
    "    # 🔹 Validation Loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iou = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks, _ in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):  # Use mixed precision in inference\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks) + dice_loss(outputs, masks)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_iou += iou_score(outputs, masks, num_classes=2)\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_val_loss = val_loss / num_batches\n",
    "    avg_val_iou = val_iou / num_batches\n",
    "\n",
    "    # 🔥 Save Best Model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), f\"bm_effdeepl_D_epoch{epoch}.pth\")\n",
    "        print(\"🔥 Best Model Saved!\")\n",
    "\n",
    "    # 🔹 Logging\n",
    "    print(f\"\\n🔹 Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"   📉 Train Loss: {avg_train_loss:.4f} | 🏆 Train IoU: {avg_train_iou:.4f}\")\n",
    "    print(f\"   📉 Val Loss: {avg_val_loss:.4f} | 🏆 Val IoU: {avg_val_iou:.4f}\")\n",
    "\n",
    "    history[\"epoch\"].append(epoch + 1)\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"train_iou\"].append(avg_train_iou)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"val_iou\"].append(avg_val_iou)\n",
    "\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_path = os.path.join(output_dir, \"training_history_02_D.csv\")\n",
    "    history_df.to_csv(history_path, index=False)\n",
    "    print(f\"📊 Training history saved to: {history_path}\")\n",
    "    \n",
    "    # 🔹 Adjust LR based on Validation Loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (caa01)",
   "language": "python",
   "name": "caa01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
