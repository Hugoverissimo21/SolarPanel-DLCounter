{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import Counter\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.backends.cudnn as cudnn\n",
    "#import os\n",
    "#cudnn.benchmark = True\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train = pd.read_pickle(r\"C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\Model_Train.pkl\")\n",
    "df_val = pd.read_pickle(r\"C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\Model_Val.pkl\")\n",
    "df_train = df_train[df_train[\"img_origin\"] == \"S\"].reset_index(drop=True)\n",
    "df_val = df_val[df_val[\"img_origin\"] == \"S\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is using a different strategy:\n",
    "- The metadata was encoded onto the images via one hot encoding \n",
    "- Based on the 2 classes and 2 origins, the class balancing was attempted for the 4 classes during the albumentations step (although officially there are only 2 classes still, solar and boiler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gnvca\\AppData\\Local\\Temp\\ipykernel_23776\\1064303010.py:20: UserWarning: Argument(s) 'max_holes, max_height, max_width, min_holes, fill_value' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=8, max_height=50, max_width=50, min_holes=4, fill_value=0, p=0.5),\n"
     ]
    }
   ],
   "source": [
    "# Function to create multi-class mask\n",
    "def create_multi_class_mask(image_size, polygons_boil, polygons_pan):\n",
    "    mask = np.full(image_size, 1, dtype=np.uint8)  # Default background is Photovoltaic (1)\n",
    "    \n",
    "    # Draw boiler panels (0)\n",
    "    for polygon in polygons_boil:\n",
    "        cv2.fillPoly(mask, np.array([polygon], dtype=np.int32), 0)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "# One-hot encode metadata\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "metadata_encoded = encoder.fit_transform(df_train[['img_placement', 'img_origin']])\n",
    "\n",
    "# Define transformation pipelines\n",
    "albumentations_transform = A.Compose([\n",
    "    A.Resize(512, 512),  # Resize first to a slightly larger size\n",
    "    A.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.8),\n",
    "    A.CoarseDropout(max_holes=8, max_height=50, max_width=50, min_holes=4, fill_value=0, p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Perspective(scale=(0.05, 0.1), p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.GaussianBlur(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Dataset class\n",
    "class SolarPanelDataset(Dataset):\n",
    "    def __init__(self, metadata_df, image_dir, transform=None, mask_size=(512, 512), balance=False):\n",
    "        self.metadata = metadata_df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.mask_size = mask_size\n",
    "        self.balance = balance\n",
    "        \n",
    "        # One-hot encode metadata\n",
    "        self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self.encoded_metadata = self.encoder.fit_transform(self.metadata[['img_placement', 'img_origin']])\n",
    "        \n",
    "        # Create class labels for balancing\n",
    "        self.class_labels = self.metadata.apply(lambda row: f\"{row['img_origin']}_{'solar' if row['polygons_pan'] else 'boiler'}\", axis=1)\n",
    "        \n",
    "        # Compute class weights for balancing\n",
    "        if balance:\n",
    "            class_counts = Counter(self.class_labels)\n",
    "            self.weights = [1.0 / class_counts[label] for label in self.class_labels]\n",
    "        else:\n",
    "            self.weights = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        img_path = f\"{self.image_dir}/{row['img_id']}.jpg\"\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        # Create the mask\n",
    "        mask = create_multi_class_mask(image.shape[:2], row['polygons_boil'], row['polygons_pan'])\n",
    "        mask = np.array(mask, dtype=np.uint8)\n",
    "\n",
    "        # Apply transformations\n",
    "        augmented = self.transform(image=image, mask=mask)\n",
    "        image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "        # Convert mask to long tensor\n",
    "        if isinstance(mask, np.ndarray):  # Convert only if it's still a NumPy array\n",
    "            mask = torch.from_numpy(mask).long()\n",
    "        else:\n",
    "            mask = mask.long()  # If it's already a tensor, just ensure dtype\n",
    "\n",
    "        # Get one-hot encoded metadata\n",
    "        metadata_vector = torch.tensor(self.encoded_metadata[idx], dtype=torch.float32)\n",
    "\n",
    "        return image, mask, metadata_vector  # Return metadata as additional input\n",
    "\n",
    "# Define image directory\n",
    "image_dir = r\"C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\images\"\n",
    "\n",
    "# Create train dataset with class balancing\n",
    "train_dataset = SolarPanelDataset(df_train, image_dir, transform=albumentations_transform, balance=True)\n",
    "val_dataset = SolarPanelDataset(df_val, image_dir, transform=A.Compose([\n",
    "    A.Resize(512, 512),  # Ensure same resize as training\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "]))\n",
    "\n",
    "# Create Weighted Sampler for class balancing\n",
    "if train_dataset.weights:\n",
    "    sampler = WeightedRandomSampler(weights=train_dataset.weights, num_samples=len(train_dataset), replacement=True) if train_dataset.weights else None\n",
    "\n",
    "num_workers = 0 if os.name == 'nt' else 4\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler if sampler else None, shuffle=sampler is None, num_workers=num_workers, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gnvca\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load DeepLabV3+ with EfficientNet-B4 backbone\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"efficientnet-b4\",  # EfficientNet-B4 as the encoder\n",
    "    encoder_weights=\"imagenet\",  # Pretrained weights\n",
    "    in_channels=3,  # RGB images\n",
    "    classes=2  # Boiler (0), Photovoltaic (1)\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Add dropout before the classifier correctly\n",
    "model.segmentation_head = nn.Sequential(\n",
    "    nn.Dropout(0.3),  # 30% dropout\n",
    "    model.segmentation_head\n",
    ")\n",
    "\n",
    "# Define loss function (CrossEntropy + Dice Loss for better performance)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dice_loss = smp.losses.DiceLoss(mode='multiclass')\n",
    "\n",
    "# Adam optimizer with weight decay\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Mixed precision scaler for faster GPU training\n",
    "scaler = torch.amp.GradScaler(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [03:57<00:00,  4.32s/it]  \n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:05<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 1/20\n",
      "   ðŸ“‰ Train Loss: 1.2376 | ðŸ† Train IoU: 0.3612\n",
      "   ðŸ“‰ Val Loss: 1.2512 | ðŸ† Val IoU: 0.3954\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:45<00:00,  1.92s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 2/20\n",
      "   ðŸ“‰ Train Loss: 1.0465 | ðŸ† Train IoU: 0.4744\n",
      "   ðŸ“‰ Val Loss: 1.0713 | ðŸ† Val IoU: 0.4860\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:44<00:00,  1.90s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 3/20\n",
      "   ðŸ“‰ Train Loss: 0.8637 | ðŸ† Train IoU: 0.4936\n",
      "   ðŸ“‰ Val Loss: 0.8981 | ðŸ† Val IoU: 0.4901\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:44<00:00,  1.90s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 4/20\n",
      "   ðŸ“‰ Train Loss: 0.7507 | ðŸ† Train IoU: 0.4947\n",
      "   ðŸ“‰ Val Loss: 0.7988 | ðŸ† Val IoU: 0.4943\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:43<00:00,  1.88s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 5/20\n",
      "   ðŸ“‰ Train Loss: 0.6905 | ðŸ† Train IoU: 0.4956\n",
      "   ðŸ“‰ Val Loss: 0.7290 | ðŸ† Val IoU: 0.4955\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:43<00:00,  1.88s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 6/20\n",
      "   ðŸ“‰ Train Loss: 0.6568 | ðŸ† Train IoU: 0.4949\n",
      "   ðŸ“‰ Val Loss: 0.6734 | ðŸ† Val IoU: 0.4969\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:42<00:00,  1.85s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 7/20\n",
      "   ðŸ“‰ Train Loss: 0.6327 | ðŸ† Train IoU: 0.4951\n",
      "   ðŸ“‰ Val Loss: 0.6449 | ðŸ† Val IoU: 0.4969\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:43<00:00,  1.89s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 8/20\n",
      "   ðŸ“‰ Train Loss: 0.6136 | ðŸ† Train IoU: 0.4957\n",
      "   ðŸ“‰ Val Loss: 0.6132 | ðŸ† Val IoU: 0.4969\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:44<00:00,  1.89s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 9/20\n",
      "   ðŸ“‰ Train Loss: 0.6001 | ðŸ† Train IoU: 0.4962\n",
      "   ðŸ“‰ Val Loss: 0.6059 | ðŸ† Val IoU: 0.4969\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:42<00:00,  1.86s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 10/20\n",
      "   ðŸ“‰ Train Loss: 0.5900 | ðŸ† Train IoU: 0.4961\n",
      "   ðŸ“‰ Val Loss: 0.5947 | ðŸ† Val IoU: 0.4969\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:43<00:00,  1.89s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 11/20\n",
      "   ðŸ“‰ Train Loss: 0.5809 | ðŸ† Train IoU: 0.4961\n",
      "   ðŸ“‰ Val Loss: 0.5818 | ðŸ† Val IoU: 0.4969\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:44<00:00,  1.90s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 12/20\n",
      "   ðŸ“‰ Train Loss: 0.5762 | ðŸ† Train IoU: 0.4950\n",
      "   ðŸ“‰ Val Loss: 0.5726 | ðŸ† Val IoU: 0.4969\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:44<00:00,  1.91s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 13/20\n",
      "   ðŸ“‰ Train Loss: 0.5661 | ðŸ† Train IoU: 0.4962\n",
      "   ðŸ“‰ Val Loss: 0.5669 | ðŸ† Val IoU: 0.4969\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:43<00:00,  1.88s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 14/20\n",
      "   ðŸ“‰ Train Loss: 0.5577 | ðŸ† Train IoU: 0.4979\n",
      "   ðŸ“‰ Val Loss: 0.5582 | ðŸ† Val IoU: 0.5036\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:41<00:00,  1.85s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 15/20\n",
      "   ðŸ“‰ Train Loss: 0.5523 | ðŸ† Train IoU: 0.5200\n",
      "   ðŸ“‰ Val Loss: 0.5511 | ðŸ† Val IoU: 0.5053\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:42<00:00,  1.87s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 16/20\n",
      "   ðŸ“‰ Train Loss: 0.5436 | ðŸ† Train IoU: 0.5296\n",
      "   ðŸ“‰ Val Loss: 0.5470 | ðŸ† Val IoU: 0.5120\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:43<00:00,  1.88s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 17/20\n",
      "   ðŸ“‰ Train Loss: 0.5347 | ðŸ† Train IoU: 0.5460\n",
      "   ðŸ“‰ Val Loss: 0.5464 | ðŸ† Val IoU: 0.5118\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:43<00:00,  1.88s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 18/20\n",
      "   ðŸ“‰ Train Loss: 0.5302 | ðŸ† Train IoU: 0.5428\n",
      "   ðŸ“‰ Val Loss: 0.5455 | ðŸ† Val IoU: 0.5087\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:44<00:00,  1.89s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 19/20\n",
      "   ðŸ“‰ Train Loss: 0.5282 | ðŸ† Train IoU: 0.5405\n",
      "   ðŸ“‰ Val Loss: 0.5405 | ðŸ† Val IoU: 0.5101\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [01:44<00:00,  1.90s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Best Model Saved!\n",
      "\n",
      "ðŸ”¹ Epoch 20/20\n",
      "   ðŸ“‰ Train Loss: 0.5191 | ðŸ† Train IoU: 0.5514\n",
      "   ðŸ“‰ Val Loss: 0.5387 | ðŸ† Val IoU: 0.5116\n",
      "ðŸ“Š Training history saved to: C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\training_history_02_S.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate IoU\n",
    "def iou_score(preds, labels, num_classes=2):\n",
    "    \"\"\"Compute IoU (Intersection over Union) for multi-class segmentation.\"\"\"\n",
    "    preds = torch.argmax(preds, dim=1)  # Convert logits to class predictions\n",
    "    iou = []\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        intersection = ((preds == cls) & (labels == cls)).sum().item()\n",
    "        union = ((preds == cls) | (labels == cls)).sum().item()\n",
    "        if union == 0:\n",
    "            iou.append(float('nan'))\n",
    "        else:\n",
    "            iou.append(intersection / union)\n",
    "\n",
    "    return np.nanmean(iou)  # Ignore NaNs if a class is missing in batch\n",
    "\n",
    "\n",
    "# ðŸ”¹ Model Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"efficientnet-b4\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=2\n",
    ").to(device)\n",
    "\n",
    "# ðŸ”¹ Add Dropout Correctly\n",
    "model.segmentation_head = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    model.segmentation_head\n",
    ")\n",
    "\n",
    "# ðŸ”¹ Loss Functions (CrossEntropy + Dice Loss)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dice_loss = smp.losses.DiceLoss(mode='multiclass')\n",
    "\n",
    "# ðŸ”¹ Optimizer & LR Scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# ðŸ”¹ Mixed Precision (Speeds up Training)\n",
    "scaler = torch.amp.GradScaler(device=\"cuda\")\n",
    "\n",
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_iou\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_iou\": []\n",
    "}\n",
    "\n",
    "# Training Hyperparameters\n",
    "num_epochs = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "accumulation_steps = 4  # Simulates larger batch size\n",
    "output_dir = r\"C:\\Users\\gnvca\\OneDrive\\Desktop\\JP\\\\\"\n",
    "\n",
    "# ðŸ”¹ Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    optimizer.zero_grad()  # Initialize gradients before accumulation\n",
    "\n",
    "    for i, (images, masks, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\", leave=True, dynamic_ncols=True)):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        if images.size(0) == 1:\n",
    "            continue  # Skip batch to avoid BatchNorm crash\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):  # Enables mixed precision\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, masks) + dice_loss(outputs, masks)  # Combined loss\n",
    "        \n",
    "        scaler.scale(loss).backward()  # Accumulate gradients\n",
    "\n",
    "        # ðŸ”¹ Only update every `accumulation_steps`\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total_iou += iou_score(outputs, masks, num_classes=2)\n",
    "        num_batches += 1\n",
    "        \n",
    "        #tqdm.write(f\"Batch {i+1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = running_loss / num_batches\n",
    "    avg_train_iou = total_iou / num_batches\n",
    "\n",
    "    # ðŸ”¹ Validation Loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iou = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks, _ in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):  # Use mixed precision in inference\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks) + dice_loss(outputs, masks)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_iou += iou_score(outputs, masks, num_classes=2)\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_val_loss = val_loss / num_batches\n",
    "    avg_val_iou = val_iou / num_batches\n",
    "\n",
    "    # ðŸ”¥ Save Best Model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), f\"bm_effdeepl_S_epoch{epoch}.pth\")\n",
    "        print(\"ðŸ”¥ Best Model Saved!\")\n",
    "\n",
    "    # ðŸ”¹ Logging\n",
    "    print(f\"\\nðŸ”¹ Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"   ðŸ“‰ Train Loss: {avg_train_loss:.4f} | ðŸ† Train IoU: {avg_train_iou:.4f}\")\n",
    "    print(f\"   ðŸ“‰ Val Loss: {avg_val_loss:.4f} | ðŸ† Val IoU: {avg_val_iou:.4f}\")\n",
    "\n",
    "    history[\"epoch\"].append(epoch + 1)\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"train_iou\"].append(avg_train_iou)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"val_iou\"].append(avg_val_iou)\n",
    "\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_path = os.path.join(output_dir, \"training_history_02_S.csv\")\n",
    "    history_df.to_csv(history_path, index=False)\n",
    "    print(f\"ðŸ“Š Training history saved to: {history_path}\")\n",
    "    \n",
    "    # ðŸ”¹ Adjust LR based on Validation Loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can't deal with the low resolution images to accurately identify areas of interest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (caa01)",
   "language": "python",
   "name": "caa01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
