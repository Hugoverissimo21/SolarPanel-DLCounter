{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# primeiro modelo: modelo 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader  # <-- Import Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_multi_class_mask(image_size, polygons_boil, polygons_pan):\n",
    "    \"\"\"\n",
    "    Create a multi-class segmentation mask where:\n",
    "    - Boilers (nr_boil) are labeled as 0\n",
    "    - Photovoltaics (nr_pan) are labeled as 1\n",
    "    - Background is labeled as 2\n",
    "    \"\"\"\n",
    "    mask = np.full(image_size, 2, dtype=np.uint8)  # Default background is 2\n",
    "\n",
    "    # Draw boiler panels with label 0\n",
    "    for polygon in polygons_boil:\n",
    "        cv2.fillPoly(mask, np.array([polygon], dtype=np.int32), 0)\n",
    "\n",
    "    # Draw photovoltaic panels with label 1\n",
    "    for polygon in polygons_pan:\n",
    "        cv2.fillPoly(mask, np.array([polygon], dtype=np.int32), 1)\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('Model_Train.pkl')\n",
    "df_val = pd.read_pickle('Model_Val.pkl')\n",
    "\n",
    "# Albumentations transformation pipeline (same for image & mask)\n",
    "albumentations_transform = A.Compose([\n",
    "    A.Resize(256, 256),  # Resize both image & mask\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.GaussianBlur(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize for DeepLabV3+\n",
    "    ToTensorV2(),  # Convert to float tensor\n",
    "])\n",
    "\n",
    "class SolarPanelDataset(Dataset):\n",
    "    def __init__(self, metadata_df, image_dir, transform=None, mask_size=(512, 512)):\n",
    "        self.metadata = metadata_df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.mask_size = mask_size  # Target size for masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        img_path = f\"{self.image_dir}/{row['img_id']}.jpg\"\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        # Ensure the mask is created with the same size as the image\n",
    "        img_height, img_width = image.shape[:2]\n",
    "        mask = create_multi_class_mask((img_height, img_width), row['polygons_boil'], row['polygons_pan'])\n",
    "        mask = np.array(mask, dtype=np.uint8)  # Ensure mask is a NumPy array\n",
    "\n",
    "        # Apply Albumentations transformations\n",
    "        augmented = self.transform(image=image, mask=mask)\n",
    "        image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "        # Convert mask to long tensor (class labels)\n",
    "        mask = torch.tensor(mask, dtype=torch.long)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define image directory\n",
    "image_dir = \"/Users/joaop.cardoso/MestradoCD/CAA/Project 1/images\"\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_dataset = SolarPanelDataset(df_train, image_dir, transform=albumentations_transform)\n",
    "val_dataset = SolarPanelDataset(df_val, image_dir, transform=A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize for DeepLabV3+\n",
    "    ToTensorV2()\n",
    "]))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\" to /Users/joaop.cardoso/.cache/torch/hub/checkpoints/efficientnet-b4-6ed6700e.pth\n",
      "100%|██████████| 74.4M/74.4M [00:03<00:00, 23.6MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepLabV3Plus(\n",
       "  (encoder): EfficientNetEncoder(\n",
       "    (_conv_stem): Conv2dStaticSamePadding(\n",
       "      3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "      (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "    )\n",
       "    (_bn0): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_blocks): ModuleList(\n",
       "      (0): MBConvBlock(\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          48, 48, kernel_size=(3, 3), stride=[1, 1], groups=48, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          48, 12, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          12, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (1): MBConvBlock(\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          24, 24, kernel_size=(3, 3), stride=(1, 1), groups=24, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          24, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (2): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (3-5): 3 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (6): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          192, 192, kernel_size=(5, 5), stride=[2, 2], groups=192, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (7-9): 3 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (10): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          336, 336, kernel_size=(3, 3), stride=[2, 2], groups=336, bias=False\n",
       "          (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (11-15): 5 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (16): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=[1, 1], groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (17-21): 5 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (22): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          160, 960, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=960, bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          960, 40, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          40, 960, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          960, 272, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (23-29): 7 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          272, 1632, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=1632, bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1632, 68, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          68, 1632, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1632, 272, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (30): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          272, 1632, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=1632, bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1632, 68, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          68, 1632, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1632, 448, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (31): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          448, 2688, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          2688, 2688, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2688, bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          2688, 112, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          112, 2688, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          2688, 448, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "    )\n",
       "    (_conv_head): Conv2dStaticSamePadding(\n",
       "      448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (static_padding): Identity()\n",
       "    )\n",
       "    (_bn1): BatchNorm2d(1792, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "    (_dropout): Dropout(p=0.4, inplace=False)\n",
       "    (_swish): MemoryEfficientSwish()\n",
       "  )\n",
       "  (decoder): DeepLabV3PlusDecoder(\n",
       "    (aspp): Sequential(\n",
       "      (0): ASPP(\n",
       "        (convs): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ASPPSeparableConv(\n",
       "            (0): SeparableConv2d(\n",
       "              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=448, bias=False)\n",
       "              (1): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): ASPPSeparableConv(\n",
       "            (0): SeparableConv2d(\n",
       "              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), groups=448, bias=False)\n",
       "              (1): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (3): ASPPSeparableConv(\n",
       "            (0): SeparableConv2d(\n",
       "              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), groups=448, bias=False)\n",
       "              (1): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (4): ASPPPooling(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (3): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (project): Sequential(\n",
       "          (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SeparableConv2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "        (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (up): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
       "    (block1): Sequential(\n",
       "      (0): Conv2d(32, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): SeparableConv2d(\n",
       "        (0): Conv2d(304, 304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=304, bias=False)\n",
       "        (1): Conv2d(304, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Load DeepLabV3+ with EfficientNet backbone\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"efficientnet-b4\",  # Choose EfficientNet-B4 as backbone\n",
    "    encoder_weights=\"imagenet\",  # Use pretrained ImageNet weights\n",
    "    in_channels=3,  # RGB images\n",
    "    classes=3  # Boiler (0), Photovoltaic (1), Background (2)\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (CrossEntropy for multi-class segmentation)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer with learning rate scheduling\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strong signs of overfitting\n",
    "\n",
    "Next strategy:\n",
    "- Use RandomSearch with *patience* to find the best hyperparameters\n",
    "- Use early stopping to avoid overfitting\n",
    "\n",
    "Second model\n",
    "- Increase polygons area to train YOLO model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Training:   0%|          | 0/610 [00:00<?, ?it/s]/var/folders/kl/mfb9z6_d1290hf_1b38hg6zr0000gn/T/ipykernel_5045/1719488051.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(mask, dtype=torch.long)\n",
      "Epoch 1/20 Training: 100%|██████████| 610/610 [42:13<00:00,  4.15s/it]\n",
      "Validation: 100%|██████████| 153/153 [04:08<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 1/20\n",
      "   📉 Train Loss: 0.1734 | 🏆 Train IoU: 0.3881\n",
      "   📉 Val Loss: 0.0348 | 🏆 Val IoU: 0.4200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Training: 100%|██████████| 610/610 [40:00<00:00,  3.93s/it]\n",
      "Validation: 100%|██████████| 153/153 [03:44<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 2/20\n",
      "   📉 Train Loss: 0.0314 | 🏆 Train IoU: 0.4223\n",
      "   📉 Val Loss: 0.0221 | 🏆 Val IoU: 0.4200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Training: 100%|██████████| 610/610 [37:16<00:00,  3.67s/it]\n",
      "Validation: 100%|██████████| 153/153 [03:45<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 3/20\n",
      "   📉 Train Loss: 0.0237 | 🏆 Train IoU: 0.4209\n",
      "   📉 Val Loss: 0.0247 | 🏆 Val IoU: 0.4264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Training: 100%|██████████| 610/610 [37:10<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 153/153 [03:44<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 4/20\n",
      "   📉 Train Loss: 0.0210 | 🏆 Train IoU: 0.4225\n",
      "   📉 Val Loss: 0.0227 | 🏆 Val IoU: 0.4273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Training: 100%|██████████| 610/610 [37:04<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 153/153 [03:45<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 5/20\n",
      "   📉 Train Loss: 0.0198 | 🏆 Train IoU: 0.4247\n",
      "   📉 Val Loss: 0.0197 | 🏆 Val IoU: 0.4200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Training: 100%|██████████| 610/610 [37:04<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 153/153 [03:45<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 6/20\n",
      "   📉 Train Loss: 0.0180 | 🏆 Train IoU: 0.4262\n",
      "   📉 Val Loss: 0.0181 | 🏆 Val IoU: 0.4259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Training: 100%|██████████| 610/610 [37:07<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 153/153 [03:44<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 7/20\n",
      "   📉 Train Loss: 0.0177 | 🏆 Train IoU: 0.4288\n",
      "   📉 Val Loss: 0.0170 | 🏆 Val IoU: 0.4228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Training: 100%|██████████| 610/610 [37:06<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 153/153 [03:45<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 8/20\n",
      "   📉 Train Loss: 0.0165 | 🏆 Train IoU: 0.4281\n",
      "   📉 Val Loss: 0.0176 | 🏆 Val IoU: 0.4230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Training: 100%|██████████| 610/610 [37:08<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 153/153 [03:44<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 9/20\n",
      "   📉 Train Loss: 0.0156 | 🏆 Train IoU: 0.4290\n",
      "   📉 Val Loss: 0.0165 | 🏆 Val IoU: 0.4206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Training: 100%|██████████| 610/610 [37:09<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 153/153 [03:43<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best Model Saved!\n",
      "\n",
      "🔹 Epoch 10/20\n",
      "   📉 Train Loss: 0.0151 | 🏆 Train IoU: 0.4286\n",
      "   📉 Val Loss: 0.0160 | 🏆 Val IoU: 0.4207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Training: 100%|██████████| 610/610 [37:09<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 153/153 [03:44<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 11/20\n",
      "   📉 Train Loss: 0.0144 | 🏆 Train IoU: 0.4309\n",
      "   📉 Val Loss: 0.0161 | 🏆 Val IoU: 0.4215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Training: 100%|██████████| 610/610 [38:59<00:00,  3.84s/it]\n",
      "Validation: 100%|██████████| 153/153 [04:05<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 12/20\n",
      "   📉 Train Loss: 0.0139 | 🏆 Train IoU: 0.4315\n",
      "   📉 Val Loss: 0.0172 | 🏆 Val IoU: 0.4232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Training: 100%|██████████| 610/610 [40:25<00:00,  3.98s/it]\n",
      "Validation: 100%|██████████| 153/153 [04:05<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 13/20\n",
      "   📉 Train Loss: 0.0148 | 🏆 Train IoU: 0.4314\n",
      "   📉 Val Loss: 0.0165 | 🏆 Val IoU: 0.4212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Training: 100%|██████████| 610/610 [40:43<00:00,  4.01s/it]\n",
      "Validation: 100%|██████████| 153/153 [04:07<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 14/20\n",
      "   📉 Train Loss: 0.0138 | 🏆 Train IoU: 0.4300\n",
      "   📉 Val Loss: 0.0174 | 🏆 Val IoU: 0.4224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 Training: 100%|██████████| 610/610 [38:23<00:00,  3.78s/it]\n",
      "Validation: 100%|██████████| 153/153 [04:03<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 15/20\n",
      "   📉 Train Loss: 0.0134 | 🏆 Train IoU: 0.4326\n",
      "   📉 Val Loss: 0.0176 | 🏆 Val IoU: 0.4237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 Training: 100%|██████████| 610/610 [44:18<00:00,  4.36s/it]\n",
      "Validation: 100%|██████████| 153/153 [04:26<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 16/20\n",
      "   📉 Train Loss: 0.0131 | 🏆 Train IoU: 0.4355\n",
      "   📉 Val Loss: 0.0179 | 🏆 Val IoU: 0.4235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 Training: 100%|██████████| 610/610 [41:08<00:00,  4.05s/it]\n",
      "Validation: 100%|██████████| 153/153 [04:03<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 17/20\n",
      "   📉 Train Loss: 0.0126 | 🏆 Train IoU: 0.4391\n",
      "   📉 Val Loss: 0.0170 | 🏆 Val IoU: 0.4228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 Training: 100%|██████████| 610/610 [41:32<00:00,  4.09s/it]\n",
      "Validation: 100%|██████████| 153/153 [04:05<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 18/20\n",
      "   📉 Train Loss: 0.0125 | 🏆 Train IoU: 0.4363\n",
      "   📉 Val Loss: 0.0173 | 🏆 Val IoU: 0.4237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 Training: 100%|██████████| 610/610 [51:12<00:00,  5.04s/it]\n",
      "Validation: 100%|██████████| 153/153 [04:07<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 19/20\n",
      "   📉 Train Loss: 0.0126 | 🏆 Train IoU: 0.4371\n",
      "   📉 Val Loss: 0.0173 | 🏆 Val IoU: 0.4246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 Training: 100%|██████████| 610/610 [41:32<00:00,  4.09s/it]\n",
      "Validation: 100%|██████████| 153/153 [04:05<00:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Epoch 20/20\n",
      "   📉 Train Loss: 0.0122 | 🏆 Train IoU: 0.4376\n",
      "   📉 Val Loss: 0.0176 | 🏆 Val IoU: 0.4236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def iou_score(preds, labels, num_classes=3):\n",
    "    \"\"\"Compute IoU (Intersection over Union) for multi-class segmentation.\"\"\"\n",
    "    preds = torch.argmax(preds, dim=1)  # Convert logits to class predictions\n",
    "    iou = []\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        intersection = ((preds == cls) & (labels == cls)).sum().item()\n",
    "        union = ((preds == cls) | (labels == cls)).sum().item()\n",
    "        if union == 0:\n",
    "            iou.append(float('nan'))\n",
    "        else:\n",
    "            iou.append(intersection / union)\n",
    "\n",
    "    return np.nanmean(iou)  # Ignore NaNs if a class is missing in batch\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Training Loop\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # Model output\n",
    "\n",
    "        loss = criterion(outputs, masks)  # Compute loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total_iou += iou_score(outputs, masks, num_classes=3)\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = running_loss / num_batches\n",
    "    avg_train_iou = total_iou / num_batches\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iou = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_iou += iou_score(outputs, masks, num_classes=3)\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_val_loss = val_loss / num_batches\n",
    "    avg_val_iou = val_iou / num_batches\n",
    "\n",
    "    # Save Best Model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"best_model_effnet.pth\")\n",
    "        print(\"🔥 Best Model Saved!\")\n",
    "\n",
    "    print(f\"\\n🔹 Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"   📉 Train Loss: {avg_train_loss:.4f} | 🏆 Train IoU: {avg_train_iou:.4f}\")\n",
    "    print(f\"   📉 Val Loss: {avg_val_loss:.4f} | 🏆 Val IoU: {avg_val_iou:.4f}\")\n",
    "\n",
    "    scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
