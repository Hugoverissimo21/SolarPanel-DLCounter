{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def predict_mask(model, image, device):\n",
    "    \"\"\"\n",
    "    Runs inference on a new image and returns the predicted segmentation mask.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    image = image.to(device).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # Model prediction\n",
    "\n",
    "    predicted_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()  # Convert to NumPy\n",
    "    return predicted_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model successfully loaded with modified segmentation head!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# üîπ Load the Model with its original structure\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"efficientnet-b4\",\n",
    "    encoder_weights=None,  # We are loading our trained weights\n",
    "    in_channels=3,\n",
    "    classes=2\n",
    ")\n",
    "\n",
    "# üîπ Load weights (no modifications yet)\n",
    "checkpoint = torch.load(\"best_model_effnet_deepl.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "# üîπ Now modify the segmentation head AFTER loading weights\n",
    "model.segmentation_head = nn.Sequential(\n",
    "    nn.Dropout(0.3),  # 30% dropout\n",
    "    model.segmentation_head  # Keep the original segmentation head\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Model successfully loaded with modified segmentation head!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: output_images/ID00rw8_pred.png\n",
      "‚úÖ Saved: output_images/ID014O6EC7_pred.png\n",
      "‚úÖ Saved: output_images/ID020cu0z_pred.png\n",
      "‚úÖ Saved: output_images/ID024YTBkLvRpQahT_pred.png\n",
      "‚úÖ Saved: output_images/ID024YTBkLvRpQahT_pred.png\n",
      "‚úÖ Saved: output_images/ID024YTBkLvRpQahT_pred.png\n",
      "‚úÖ Saved: output_images/ID02vByTw8Htl_pred.png\n",
      "‚úÖ Saved: output_images/ID04S4rM9Qs_pred.png\n",
      "‚úÖ Saved: output_images/ID04S4rM9Qs_pred.png\n",
      "‚úÖ Saved: output_images/ID06Zb8HWf7XPxodI_pred.png\n",
      "‚úÖ Saved: output_images/ID08l4qXO27c08_pred.png\n",
      "‚úÖ Saved: output_images/ID08lsWcpi7_pred.png\n",
      "‚úÖ Saved: output_images/ID09o6w2iKE7RG_pred.png\n",
      "‚úÖ Saved: output_images/ID09pzWgf_pred.png\n",
      "‚úÖ Saved: output_images/ID09pzWgf_pred.png\n",
      "‚úÖ Saved: output_images/ID09pzWgf_pred.png\n",
      "‚úÖ Saved: output_images/ID09pzWgf_pred.png\n",
      "‚úÖ Saved: output_images/ID09pzWgf_pred.png\n",
      "‚úÖ Saved: output_images/ID09pzWgf_pred.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m predicted_mask = predict_mask(model, image, device, original_size)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Convert to BGR image format for OpenCV\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m original_image = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Create an RGB mask overlay\u001b[39;00m\n\u001b[32m     60\u001b[39m overlay = np.zeros_like(original_image, dtype=np.uint8)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/masters_clean/lib/python3.12/site-packages/PIL/Image.py:747\u001b[39m, in \u001b[36mImage.__array_interface__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    745\u001b[39m     new[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.tobytes(\u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m747\u001b[39m     new[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    748\u001b[39m new[\u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m], new[\u001b[33m\"\u001b[39m\u001b[33mtypestr\u001b[39m\u001b[33m\"\u001b[39m] = _conv_type_shape(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/masters_clean/lib/python3.12/site-packages/PIL/Image.py:817\u001b[39m, in \u001b[36mImage.tobytes\u001b[39m\u001b[34m(self, encoder_name, *args)\u001b[39m\n\u001b[32m    814\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mencoder error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merrcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in tobytes\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    815\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# üîπ Define Paths\n",
    "image_dir = \"/Users/joaop.cardoso/MestradoCD/CAA/Project 1/images\"\n",
    "test_csv = \"Train.csv\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "# üîπ Image Preprocessing (Matches Training Pipeline)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Resize to match model input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# üîπ Prediction Function\n",
    "def predict_mask(model, image, device, original_size):\n",
    "    image = image.to(device).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # Model prediction\n",
    "\n",
    "    predicted_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()  # Convert to NumPy\n",
    "\n",
    "    # üîπ Resize mask back to original image size\n",
    "    predicted_mask = cv2.resize(predicted_mask, (original_size[1], original_size[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    return predicted_mask\n",
    "\n",
    "# üîπ Read Test Image IDs\n",
    "df_test = pd.read_csv(test_csv)\n",
    "\n",
    "# üîπ Process Each Image in Test.csv\n",
    "for index, row in df_test.iterrows():\n",
    "    image_id = row[\"ID\"]  # Extract image ID\n",
    "    \n",
    "    image_path = os.path.join(image_dir, f\"{image_id}.jpg\")\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"‚ùå Image {image_id}.jpg not found, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load and preprocess image\n",
    "    original_image = Image.open(image_path).convert(\"RGB\")\n",
    "    original_size = original_image.size[::-1]  # (height, width)\n",
    "    \n",
    "    image = transform(original_image)  # Apply transformations\n",
    "\n",
    "    # Get Predictions\n",
    "    predicted_mask = predict_mask(model, image, device, original_size)\n",
    "\n",
    "    # Convert to BGR image format for OpenCV\n",
    "    original_image = np.array(original_image)\n",
    "\n",
    "    # Create an RGB mask overlay\n",
    "    overlay = np.zeros_like(original_image, dtype=np.uint8)\n",
    "\n",
    "    # Apply colors based on predicted mask\n",
    "    overlay[predicted_mask == 0] = (0, 165, 255)  # Orange (Boiler)\n",
    "    overlay[predicted_mask == 1] = (0, 255, 255)  # Yellow (Solar Panel)\n",
    "\n",
    "    # Blend the overlay with the original image\n",
    "    blended_image = cv2.addWeighted(original_image, 0.6, overlay, 0.4, 0)\n",
    "\n",
    "    # Save the resulting image\n",
    "    output_path = os.path.join(output_dir, f\"{image_id}_pred.png\")\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(blended_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"‚úÖ Saved: {output_path}\")\n",
    "\n",
    "print(f\"\\nüéâ Processing complete! All predicted images are saved in `{output_dir}`.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
